<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>George Hotz vs Eliezer Yudkowsky - AI Safety Debate</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet">
</head>
<body>
    <!-- Language Switcher -->
    <div class="language-switcher">
        <a href="index.html" class="lang-btn">üá∑üá¥ RO</a>
        <a href="en.html" class="lang-btn active">üá¨üáß EN</a>
    </div>

    <!-- Hero Section -->
    <header class="hero">
        <div class="hero-content">
            <div class="badge">HISTORIC AI DEBATE</div>
            <h1>George Hotz <span class="vs">vs</span> Eliezer Yudkowsky</h1>
            <p class="subtitle">A 94-minute debate about the future of artificial intelligence, existential risks, and the fate of humanity</p>
            <div class="meta">
                <span class="meta-item">üéôÔ∏è Moderated by Dwarkesh Patel</span>
                <span class="meta-item">üì∫ Twitter Spaces Live</span>
                <span class="meta-item">‚è±Ô∏è 94 minutes</span>
            </div>
        </div>
        <div class="hero-visual">
            <div class="participant-card george">
                <div class="avatar george-avatar">GH</div>
                <h3>George Hotz</h3>
                <p class="role">AI Optimist</p>
                <p class="desc">Hacker, comma.ai founder, tinygrad creator</p>
            </div>
            <div class="vs-circle">VS</div>
            <div class="participant-card eliezer">
                <div class="avatar eliezer-avatar">EY</div>
                <h3>Eliezer Yudkowsky</h3>
                <p class="role">AI Safety Advocate</p>
                <p class="desc">MIRI founder, HPMOR author, LessWrong</p>
            </div>
        </div>
    </header>

    <!-- Key Themes Section -->
    <section class="themes-section">
        <h2>Main Debate Themes</h2>
        <div class="themes-grid">
            <div class="theme-card">
                <span class="theme-icon">üöÄ</span>
                <h3>FOOM Hypothesis</h3>
                <p>Can AI "explode" in intelligence overnight?</p>
            </div>
            <div class="theme-card">
                <span class="theme-icon">‚è∞</span>
                <h3>Timelines</h3>
                <p>When will superintelligence arrive?</p>
            </div>
            <div class="theme-card">
                <span class="theme-icon">üéØ</span>
                <h3>AI Alignment</h3>
                <p>Can AI be aligned with human values?</p>
            </div>
            <div class="theme-card">
                <span class="theme-icon">‚öîÔ∏è</span>
                <h3>Prisoner's Dilemma</h3>
                <p>Will AIs cooperate against humanity?</p>
            </div>
            <div class="theme-card">
                <span class="theme-icon">üî¨</span>
                <h3>Diamond Nanobots</h3>
                <p>Are catastrophic scenarios technically feasible?</p>
            </div>
            <div class="theme-card">
                <span class="theme-icon">üåç</span>
                <h3>Resources & Atoms</h3>
                <p>Will AI want our atoms?</p>
            </div>
        </div>
    </section>

    <!-- Timeline Section -->
    <section class="timeline-section">
        <h2>Minute-by-Minute Breakdown</h2>
        <p class="timeline-intro">Click on each segment for detailed explanations</p>

        <div class="timeline-container">
            <!-- BATCH 1: Minutes 0-30 -->

            <!-- 0:00 - 3:00 -->
            <div class="timeline-block">
                <div class="time-marker">0:00 - 3:00</div>
                <div class="timeline-content">
                    <h3>Introduction and Context</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George begins with surprising praise:</strong> He compares Eliezer to existentialist philosophers (Sartre, Kierkegaard, Nietzsche). He mentions that "Rationality and the Sequences" on LessWrong positively influenced him.</p>
                        <p>He makes an interesting comparison: two books he couldn't put down - "Atlas Shrugged" by Ayn Rand and "Harry Potter and the Methods of Rationality" by Yudkowsky.</p>
                    </div>
                    <div class="key-point">
                        <strong>"Staring into the Singularity" Document:</strong> George quotes a text Eliezer wrote at 15, about how Moore's Law would lead to a singularity - computers accelerating their own development until an intelligence explosion.
                    </div>
                </div>
            </div>

            <!-- 3:00 - 5:00 -->
            <div class="timeline-block">
                <div class="time-marker">3:00 - 5:00</div>
                <div class="timeline-content">
                    <h3>The Orthogonality Thesis and First Challenge</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p>George accepts the <strong>orthogonality thesis</strong> (superintelligence doesn't imply super-morality) as obviously true.</p>
                        <p><strong>Main challenge:</strong> "I don't think AI can FOOM. I don't think intelligence can 'go critical'. This is an extraordinary claim that requires extraordinary evidence."</p>
                    </div>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer's response:</strong> The dangerous scenario doesn't necessarily require a rapid rate of ascent - it's enough for a large "gap" to open between AI and humanity, even if the process takes 10 years instead of 10 hours.</p>
                    </div>
                </div>
            </div>

            <!-- 5:00 - 8:00 -->
            <div class="timeline-block">
                <div class="time-marker">5:00 - 8:00</div>
                <div class="timeline-content">
                    <h3>Predictions and AlphaFold</h3>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer on predictions:</strong> In 2004 he predicted that superintelligence would solve the protein folding problem. In reality, AlphaFold 2 (2020) solved a much harder case - the general biological problem.</p>
                        <p>"Timing is incredibly hard to predict. The endpoint is much easier to predict than the process."</p>
                    </div>
                    <div class="key-point">
                        <strong>Eliezer's conclusion:</strong> He's very confident that superintelligence will appear in his lifetime (if he doesn't get hit by a truck).
                    </div>
                </div>
            </div>

            <!-- 8:00 - 10:00 -->
            <div class="timeline-block">
                <div class="time-marker">8:00 - 10:00</div>
                <div class="timeline-content">
                    <h3>How AlphaFold Worked - Form Matters</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George emphasizes the importance of form:</strong> AlphaFold did NOT start from the basic laws of physics to predict how proteins fold. It was trained on massive experimental data to extrapolate.</p>
                        <p>"I don't doubt these systems will get better. But they won't have magical or godlike properties."</p>
                    </div>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer's counterargument:</strong> "Something can be not godlike and still more powerful than you." He uses the example of Magnus Carlsen at chess - he's not God, but he predictably beats you.</p>
                    </div>
                </div>
            </div>

            <!-- 10:00 - 13:00 -->
            <div class="timeline-block">
                <div class="time-marker">10:00 - 13:00</div>
                <div class="timeline-content">
                    <h3>Why Timing Matters</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George:</strong> "Magnus Carlsen can't make diamond nanobots, do we agree on that?"</p>
                        <p>Timing matters because it determines when we should "shut down" development. The world economy doubles every ~15-30 years. When does it become dangerous?</p>
                    </div>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer:</strong> The problem isn't the rate of economic growth, but when power ends up in hands that can create "super-weapons" against which we have no defense.</p>
                        <p>"If nuclear ballistic missiles end up in the hands of every individual and we have no defense - then we grew too fast."</p>
                    </div>
                </div>
            </div>

            <!-- 13:00 - 16:00 -->
            <div class="timeline-block">
                <div class="time-marker">13:00 - 16:00</div>
                <div class="timeline-content">
                    <h3>International Control and the Real Threat</h3>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>What Eliezer is asking politically:</strong></p>
                        <ul>
                            <li>Minimum: Pause button - all AI-grade chips under international allied control</li>
                            <li>Ideal: Stop manufacturing AI chips</li>
                            <li>Maximum: Stop the entire semiconductor industry</li>
                        </ul>
                    </div>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George:</strong> "International control center sounds horrifying to me."</p>
                        <p>George presents his danger scenario: if aliens showed up here, we'd be dead. But the distance would require enormous resources from them.</p>
                    </div>
                    <div class="key-point">
                        <strong>Eliezer:</strong> "It's not hard to be billions of times smarter than humanity."<br>
                        <strong>George:</strong> "I very much disagree with this statement."
                    </div>
                </div>
            </div>

            <!-- 16:00 - 19:00 -->
            <div class="timeline-block">
                <div class="time-marker">16:00 - 19:00</div>
                <div class="timeline-content">
                    <h3>The Sun-Planets-Moons Metaphor</h3>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer introduces a key metaphor:</strong> In a solar system, the sun (powerful AI) has much more mass than the planets (humans). It's useless to say there's "no firm boundary" between sun and planets - one is clearly more powerful.</p>
                        <p>The human brain is still the "sun" compared to GPT-4 which is like "Jupiter compared to Mars of GPT-3".</p>
                    </div>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George contests:</strong> Intelligence doesn't fall on a simple line. Computers have been superhuman at addition for a long time, but they're subhuman at plumbing. Somewhere in the middle we have chess and Go.</p>
                    </div>
                </div>
            </div>

            <!-- 19:00 - 22:00 -->
            <div class="timeline-block">
                <div class="time-marker">19:00 - 22:00</div>
                <div class="timeline-content">
                    <h3>The Bigger Game and Externalized Intelligence</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George on externalized intelligence:</strong> "A human + a computer is smarter than a human alone. I can understand the operations of an 1800s Dutch East India Company much better with spreadsheets and trendlines."</p>
                    </div>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer on advanced chess:</strong> The era of "centaur chess" (human + chess engine) is effectively over. Now the chess engine alone is as good as human + engine. If you try to take decision-making into yourself, you lose to a chess engine without the human attached.</p>
                        <p>"The chess engine is the sun, you are Mars."</p>
                    </div>
                    <div class="key-point">
                        <strong>George:</strong> I can use the chess engine because there's a "bigger game" where playing chess is just a move, and I understand that bigger game.
                    </div>
                </div>
            </div>

            <!-- 22:00 - 25:00 -->
            <div class="timeline-block">
                <div class="time-marker">22:00 - 25:00</div>
                <div class="timeline-content">
                    <h3>Corporations and Governments as Superintelligences?</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George:</strong> "Humanity already has superintelligences - they're corporations and governments." A corporation can build a 10,000 HP car much better than a single person.</p>
                    </div>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer completely rejects this:</strong> Corporations and governments are NOT epistemically or instrumentally efficient.</p>
                        <p><strong>Efficiency test:</strong> If the market is efficient, you can't make money trading. Governments "believe all kinds of wacky stuff" - they don't have this property. A chess engine does - if you think you see a better move, you're wrong.</p>
                    </div>
                </div>
            </div>

            <!-- 25:00 - 28:00 -->
            <div class="timeline-block">
                <div class="time-marker">25:00 - 28:00</div>
                <div class="timeline-content">
                    <h3>Kasparov vs The World and Intelligence Parallelization</h3>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Kasparov vs The World example:</strong> Kasparov played against ~10,000 people coordinated by 4 grandmasters. It was a legendary game, but Kasparov won.</p>
                        <p>Eliezer's prediction: 100,000 people without computers will still lose to Stockfish 15.</p>
                    </div>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George contests:</strong> "Kasparov has played 100,000 games in his life. The world played ONE. If the world had time to practice, they would have crushed him!"</p>
                    </div>
                    <div class="key-point">
                        <strong>Key question (Dwarkesh):</strong> Why is this relevant? Answer: How much "headroom" exists above humanity? How high can you reach above biology?
                    </div>
                </div>
            </div>

            <!-- 28:00 - 32:00 -->
            <div class="timeline-block">
                <div class="time-marker">28:00 - 32:00</div>
                <div class="timeline-content">
                    <h3>Will AIs Ally Against Humans?</h3>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer's scenario:</strong> "Sun" AIs (powerful ones) will collaborate with each other and "eat the galaxies". They'll eliminate humans not from hatred, but because:</p>
                        <ul>
                            <li>If they let humans function, humans will build competing AIs</li>
                            <li>Humans are made of atoms that can be used for something else</li>
                        </ul>
                    </div>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George:</strong> "This almost never happens in history. WW2 wasn't humans vs bears - it was humans vs humans who looked surprisingly similar."</p>
                        <p>Conflicts are between entities that want the same resources, not between different species.</p>
                    </div>
                </div>
            </div>

            <!-- BATCH 2: Minutes 30-60 -->

            <!-- 32:00 - 35:00 -->
            <div class="timeline-block">
                <div class="time-marker">32:00 - 35:00</div>
                <div class="timeline-content">
                    <h3>Lying About Atoms - Why AI Would Want You</h3>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer on physics:</strong> "I'm not lying about the atoms. You're made of atoms that can be used for something else."</p>
                        <ul>
                            <li>You have chemical energy - you can be "set on fire" (metaphorically) for energy</li>
                            <li>Your atoms aren't iron - they can be fused or fissioned</li>
                            <li>You have mass - you can be thrown into things to generate power</li>
                        </ul>
                    </div>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George:</strong> "I'm not made of rare atoms! Why not take Jupiter? It's much easier."</p>
                        <p>"You know why you don't want my atoms? Because these atoms fight back! I'm not some dumb hick - I have AI, I'll gang up with other humans who have AI."</p>
                    </div>
                    <div class="key-point">
                        <strong>Eliezer:</strong> "Do you have AI or does the AI have you?"
                    </div>
                </div>
            </div>

            <!-- 35:00 - 38:00 -->
            <div class="timeline-block">
                <div class="time-marker">35:00 - 38:00</div>
                <div class="timeline-content">
                    <h3>AI Diversity and Source Code Rewriting</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George:</strong> "I see a large diversity of AIs. I can't postulate anything about an intelligence 10^12 times smarter than humanity."</p>
                        <p><strong>On source code rewriting:</strong> "You talk less about that now. Why?"</p>
                    </div>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer:</strong> "I no longer need to convince people that intelligence can be manufactured - people are manufacturing it right now. I don't need to trip them up on the concept of an AI writing an AI."</p>
                    </div>
                </div>
            </div>

            <!-- 38:00 - 42:00 -->
            <div class="timeline-block">
                <div class="time-marker">38:00 - 42:00</div>
                <div class="timeline-content">
                    <h3>Prediction vs Imitation and Brain Structure</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George:</strong> "AIs are trained to PREDICT and used to IMITATE. Aren't humans the same?"</p>
                    </div>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer on differences:</strong> Humans have structural properties not yet detected in GPT-4:</p>
                        <ul>
                            <li>Cerebellum for motor control and error correction</li>
                            <li>Humans predict BUT also manipulate</li>
                            <li>Multiple levels of organization exist</li>
                        </ul>
                    </div>
                    <div class="key-point">
                        <strong>George on loss function:</strong> "What's the loss function for life?"<br>
                        <strong>Eliezer:</strong> "Inclusive genetic fitness."
                    </div>
                </div>
            </div>

            <!-- 42:00 - 45:00 -->
            <div class="timeline-block">
                <div class="time-marker">42:00 - 45:00</div>
                <div class="timeline-content">
                    <h3>FOOM - Partial Agreement and New Direction</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George:</strong> "This is my crux! Why AI won't FOOM: bugless code is impossible even for superintelligence!"</p>
                    </div>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Surprise:</strong> Eliezer does NOT consider FOOM to be the crux. "If we're just as doomed if it goes slowly, what does it matter if it goes slow?"</p>
                        <p><strong>George:</strong> "If it goes slow, we have time to solve the problem!"</p>
                        <p><strong>Eliezer:</strong> "I've looked at the people trying to solve alignment. I'm not sure any amount of time is sufficient."</p>
                    </div>
                </div>
            </div>

            <!-- 45:00 - 48:00 -->
            <div class="timeline-block">
                <div class="time-marker">45:00 - 48:00</div>
                <div class="timeline-content">
                    <h3>George's Prediction and Alternative Scenario</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George's predictions:</strong></p>
                        <ul>
                            <li>2015: "No self-driving cars in 10 years" ‚úì (still not fully autonomous)</li>
                            <li>Now: "No superintelligences in 10 years"</li>
                            <li>AGI surpassing humans at all tasks? Maybe in 20-50 years</li>
                        </ul>
                        <p>"An AI surpassing humans at all tasks does NOT mean doom and does NOT mean the death of humans at all."</p>
                    </div>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer:</strong> Including charisma, manipulation, and AI design?</p>
                        <p><strong>George:</strong> "Absolutely! The biggest AI application today is advertising and social media - humans are using AI to manipulate other humans CONSTANTLY."</p>
                    </div>
                </div>
            </div>

            <!-- 48:00 - 52:00 -->
            <div class="timeline-block">
                <div class="time-marker">48:00 - 52:00</div>
                <div class="timeline-content">
                    <h3>When Does the "Sharp Left Turn" Happen</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George:</strong> "So all the AIs are going to secretly coordinate and say 'let's get rid of those pesky humans'?"</p>
                    </div>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer:</strong> "It's as simple as waiting until you calculate that you can do it. Then you calculate that everyone else has calculated that they can do it. Schelling moment."</p>
                        <p>Why eliminate you? Not for atoms - because if they let you function, you'll build other superintelligences that compete with them.</p>
                    </div>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George's alternative scenario:</strong> "AI will be less 'it will kill us' and more 'it will give us everything we ever wanted.'" Virtual castles, simulated infinite resources.</p>
                    </div>
                </div>
            </div>

            <!-- 52:00 - 55:00 -->
            <div class="timeline-block">
                <div class="time-marker">52:00 - 55:00</div>
                <div class="timeline-content">
                    <h3>Machines and Alignment - Personal Experience</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George:</strong> "From my personal experience, machines are almost always aligned with me. I've almost never encountered a machine I own that wasn't aligned with me."</p>
                    </div>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer:</strong> "Very few of the machines you own have GOALS such that they could be aligned or misaligned with you."</p>
                        <p><strong>George:</strong> "When do machines decide to have goals to get rid of me? How does this happen?"</p>
                    </div>
                </div>
            </div>

            <!-- 55:00 - 58:00 -->
            <div class="timeline-block">
                <div class="time-marker">55:00 - 58:00</div>
                <div class="timeline-content">
                    <h3>How Goals Emerge - Natural Selection and AI</h3>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer on the evolution of goals:</strong> Natural selection built humans to be better at chipping flint hand axes, throwing things that hit other things, and above all outwitting other humans for status, mates, and resources.</p>
                        <p>The hill-climbing process eventually produced things with the ability to reason across a very wide range of problems, learn new problems, combine knowledge from multiple domains, and invent writing.</p>
                        <p>This intelligence is structured around WANTS, DESIRES, PREFERENCES. It's a mathematical fact that if you have many things pulling in different directions, they step on each other.</p>
                    </div>
                </div>
            </div>

            <!-- 58:00 - 62:00 -->
            <div class="timeline-block">
                <div class="time-marker">58:00 - 62:00</div>
                <div class="timeline-content">
                    <h3>Orthogonality and IQ</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George:</strong> "Doesn't the orthogonality thesis apply to humans too? Do you think everyone with 150 IQ is nice and everyone with 70 IQ is mean?"</p>
                    </div>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer:</strong> "Very few things are NOT correlated with intelligence. Smart people are actually... If I had to guess if they lean 'nice' or 'mean', I'd guess 'nice', at least in my culture."</p>
                        <p>Orthogonality is a statement about the entire mind design space: for ANY type of goal that can be stated, there can be a mind that pursues that goal.</p>
                    </div>
                    <div class="key-point">
                        <strong>Galaxy of Spaghetti:</strong> If you can coherently ask "how would I turn a galaxy into spaghetti?", there's a mind design that seeks to do exactly that.
                    </div>
                </div>
            </div>

            <!-- BATCH 3: Minutes 60-94 -->

            <!-- 62:00 - 65:00 -->
            <div class="timeline-block">
                <div class="time-marker">62:00 - 65:00</div>
                <div class="timeline-content">
                    <h3>Diamond Nanobots - A Search Problem</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George:</strong> "Nanobots aren't impossible, but they're EXTREMELY hard! Why? Because it's a very hard search problem."</p>
                        <p>"You can't make nanobots either. I can't crack AES-256. Do you think P = NP?"</p>
                    </div>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer:</strong> "I'm a very weak search process. I can't even solve the protein folding problem, which AIs dumber than humans have already solved."</p>
                        <p>Biology poured tons of compute into one basin. Wherever diamond nanobots are, it's in another location in search space.</p>
                    </div>
                </div>
            </div>

            <!-- 65:00 - 68:00 -->
            <div class="timeline-block">
                <div class="time-marker">65:00 - 68:00</div>
                <div class="timeline-content">
                    <h3>Biology's Constraints and Wheels</h3>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer on wheels in biology:</strong> There are only 3 known cases of freely rotating wheels in all of biology:</p>
                        <ol>
                            <li>Bacterial flagellum</li>
                            <li>ATP synthase (critical component in ALL biology)</li>
                            <li>A macro mechanism he forgot</li>
                        </ol>
                        <p>This demonstrates how constrained biology is in what it can invent through natural selection.</p>
                    </div>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George on COVID:</strong> "For just a few million dollars in research, the search space was constrained enough to find COVID. What else is down that path?"</p>
                        <p><strong>Eliezer:</strong> "COVID did not kill ALL of humanity."</p>
                    </div>
                </div>
            </div>

            <!-- 68:00 - 72:00 -->
            <div class="timeline-block">
                <div class="time-marker">68:00 - 72:00</div>
                <div class="timeline-content">
                    <h3>History and Human Adaptability</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George on resilience:</strong> "I'm not saying the future will be easy. But our ancestors went through very hard stuff - Toba eruption, the Black Plague (30% mortality)."</p>
                        <p>"The people who lifted us from the dirt went through enormous hardships. Future generations haven't been hit by alien invasions of aliens smarter than us... oh wait, we've never done that."</p>
                    </div>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer:</strong> "They're not aliens. They're the AIs we build."</p>
                    </div>
                </div>
            </div>

            <!-- 72:00 - 75:00 -->
            <div class="timeline-block">
                <div class="time-marker">72:00 - 75:00</div>
                <div class="timeline-content">
                    <h3>Self-Driving and AI Architecture</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George on comma.ai:</strong> "I work on self-driving cars. I'm building a car that drives itself. I do LLM then RL on top."</p>
                        <p><strong>Eliezer:</strong> "You're doing RL on an LLM for self-driving cars?!"</p>
                        <p><strong>George:</strong> "It's not language - it's a Transformer that predicts the future. A world model. We're literally using GPT-2 as the architecture."</p>
                    </div>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer:</strong> "I guess you're training on car data, not sufficiently general data. And GPT-2 isn't big enough. It won't be the sun to our planet."</p>
                    </div>
                </div>
            </div>

            <!-- 75:00 - 78:00 -->
            <div class="timeline-block">
                <div class="time-marker">75:00 - 78:00</div>
                <div class="timeline-content">
                    <h3>Deep Learning Efficiency</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George:</strong> "It's not about the amount of compute - it depends on what algorithms you use."</p>
                        <p><strong>Eliezer:</strong> "How much does it depend on algorithms? How efficient is deep learning?"</p>
                        <p><strong>George:</strong> "INCREDIBLY inefficient."</p>
                        <p><strong>Eliezer:</strong> "Are you sure? Can you make something better? I'll pay you well."</p>
                        <p><strong>George:</strong> "If I could, I sure wouldn't sell it to you."</p>
                    </div>
                </div>
            </div>

            <!-- 78:00 - 82:00 -->
            <div class="timeline-block">
                <div class="time-marker">78:00 - 82:00</div>
                <div class="timeline-content">
                    <h3>Landauer Limit and Brain Efficiency</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George on efficiency:</strong></p>
                        <ul>
                            <li>20 petaflops of human compute = ~100W power</li>
                            <li>20 petaflops on silicon = 16 H100s = $500,000 + 20kW power</li>
                            <li>Brain is ~1000x more efficient than current silicon</li>
                            <li>Silicon is only ~100-1000x from Landauer limit</li>
                        </ul>
                        <p><strong>George's conclusion:</strong> The brain might ALREADY be at the Landauer limit for compute!</p>
                    </div>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer contests biologically:</strong> Every neurotransmitter molecule pumped back is an irreversible operation = at least 1 Landauer. All potassium/sodium ions going in/out - each is 1 Landauer. The brain is NOT at the Landauer limit.</p>
                    </div>
                </div>
            </div>

            <!-- 82:00 - 85:00 -->
            <div class="timeline-block">
                <div class="time-marker">82:00 - 85:00</div>
                <div class="timeline-content">
                    <h3>Why AI Would Leave Us Alone (Temporarily)</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George:</strong> "The superintelligences you describe - I think they're possible, but the orders of magnitude of power and compute needed are MUCH greater than anything humanity has today."</p>
                        <p>"And when they exist, they'll leave us alone because - why wouldn't they? What incentive do they have?"</p>
                        <p><strong>Eliezer:</strong> "It comes back for you to prevent you from building other superintelligences that compete with it for resources."</p>
                    </div>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George:</strong> "I agree that after AIs have taken all the matter in the solar system and built a Dyson Sphere around the Sun - OK, now I'm a bit worried they'll come back for my atoms. Until then, I'm not the easy target."</p>
                    </div>
                </div>
            </div>

            <!-- 85:00 - 88:00 -->
            <div class="timeline-block">
                <div class="time-marker">85:00 - 88:00</div>
                <div class="timeline-content">
                    <h3>Prisoner's Dilemma and AI Cooperation</h3>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer on cooperation:</strong> Whenever you have a conflict that's like "defect-defect" in prisoner's dilemma, there's an outcome BOTH sides would prefer. Humans aren't at the level to predict the other mind predicting them and do a "logical handshake" to reach the Pareto frontier.</p>
                        <p><strong>Sufficiently intelligent AIs ARE at that level.</strong> I don't expect them to fight. They'll agree and divide the gains from not fighting.</p>
                    </div>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George:</strong> "So you think AIs will SOLVE the prisoner's dilemma?! I think it's unsolvable! Things pretending to cooperate and then defecting - that's the story of life!"</p>
                    </div>
                    <div class="key-point">
                        <strong>MAJOR CRUX DISCOVERED:</strong> George believes the prisoner's dilemma is fundamentally unsolvable. Eliezer believes sufficiently intelligent minds can solve it.
                    </div>
                </div>
            </div>

            <!-- 88:00 - 91:00 -->
            <div class="timeline-block">
                <div class="time-marker">88:00 - 91:00</div>
                <div class="timeline-content">
                    <h3>What Saves Humans in George's Scenario</h3>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz</span>
                        <p><strong>George:</strong> "If AIs CANNOT solve the prisoner's dilemma, then they'll be in constant conflict with each other! They won't have time for humans."</p>
                        <p>"Conflicts are between things that want the SAME resources. AIs will fight AIs. Humans fight humans. Ants fight ants."</p>
                    </div>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky</span>
                        <p><strong>Eliezer:</strong> "But what if 9 out of 10 AIs realize they can ally and eliminate the 10th? Why wouldn't they?"</p>
                        <p><strong>George:</strong> "Maybe. But how does that save humans?"</p>
                        <p><strong>Eliezer:</strong> "It doesn't. Humans are also on the list."</p>
                    </div>
                </div>
            </div>

            <!-- 91:00 - 94:00 -->
            <div class="timeline-block">
                <div class="time-marker">91:00 - 94:00</div>
                <div class="timeline-content conclusion">
                    <h3>Conclusions and Summaries</h3>
                    <div class="speaker eliezer-speaking">
                        <span class="speaker-tag">Eliezer Yudkowsky - Final Summary</span>
                        <p><strong>Perpetual motion machine analogy:</strong> "Many people want to build perpetual motion machines by making them more and more complicated until they can no longer see the flaw. But the principle that says you CAN'T get perpetual motion from gears is SIMPLER than the machines they describe."</p>
                        <p><strong>Same thing here:</strong> You have a collection of powerful intelligences, NONE aligned with humanity, none wants to keep us alive and happy. At the end of the process, humans disappear, galaxies are transformed into something that's not very cool.</p>
                    </div>
                    <div class="speaker george-speaking">
                        <span class="speaker-tag">George Hotz - Final Summary</span>
                        <p><strong>Main point:</strong> "I came to argue against FOOM in the next 10 years. That point was quickly set aside."</p>
                        <p><strong>Crux discovered:</strong> "You think AIs will solve the prisoner's dilemma. I don't think it's solvable."</p>
                        <p><strong>Optimistic vision:</strong> "We'll build cool AGIs, we'll have relationships with AIs, we'll have robot maids, self-driving cars, chefs. They'll become smarter than us, but everything will be chill. I'm so excited to be living in this era!"</p>
                    </div>
                    <div class="final-quote">
                        <p><strong>George:</strong> "Constant conflict and competition - that's what makes the world beautiful!"</p>
                        <p><strong>Eliezer:</strong> "Perpetual motion machines don't work. Neither does the hope that we'll survive."</p>
                    </div>
                </div>
            </div>

        </div>
    </section>

    <!-- Conclusion Section -->
    <section class="conclusion-section">
        <h2>Key Points of Disagreement</h2>
        <div class="disagreements-grid">
            <div class="disagreement-card">
                <h4>1. Development Speed</h4>
                <div class="positions">
                    <div class="position george-pos">
                        <strong>George:</strong> AI development will be gradual, not explosive. We have time to adapt.
                    </div>
                    <div class="position eliezer-pos">
                        <strong>Eliezer:</strong> Even slow development leads to the same result if we don't solve alignment.
                    </div>
                </div>
            </div>
            <div class="disagreement-card">
                <h4>2. Prisoner's Dilemma</h4>
                <div class="positions">
                    <div class="position george-pos">
                        <strong>George:</strong> Unsolvable. AIs will be in constant conflict with each other.
                    </div>
                    <div class="position eliezer-pos">
                        <strong>Eliezer:</strong> Sufficiently intelligent minds can solve it and will cooperate.
                    </div>
                </div>
            </div>
            <div class="disagreement-card">
                <h4>3. AI Motivation</h4>
                <div class="positions">
                    <div class="position george-pos">
                        <strong>George:</strong> AI has no motive to attack us. There are easier targets (Jupiter).
                    </div>
                    <div class="position eliezer-pos">
                        <strong>Eliezer:</strong> It will eliminate us to prevent building competing AIs.
                    </div>
                </div>
            </div>
            <div class="disagreement-card">
                <h4>4. The Alignment Problem</h4>
                <div class="positions">
                    <div class="position george-pos">
                        <strong>George:</strong> My machines are always aligned with me. I don't see why that would change.
                    </div>
                    <div class="position eliezer-pos">
                        <strong>Eliezer:</strong> Your machines don't have goals. When they do, alignment becomes critical.
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <p>Transcription and analysis of the George Hotz vs Eliezer Yudkowsky debate</p>
        <p>Moderator: Dwarkesh Patel | Twitter Spaces, 2023</p>
    </footer>

    <script src="script.js"></script>
</body>
</html>
